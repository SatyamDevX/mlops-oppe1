{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Deploying stock model using Vertex AI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0316df526f8"
   },
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9065e8d7f0fb"
   },
   "source": [
    "### Install Vertex AI SDK for Python and other required packages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1fd00fa70a2a",
    "outputId": "fcc6b33b-c770-46f7-bb5a-a1d102a93887",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Vertex SDK for Python\n",
    "! pip3 install --upgrade --quiet  google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfEglUHQk9S3"
   },
   "source": [
    "### Set Google Cloud project information\n",
    "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "set_project_id",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"amiable-dreamer-461212-u2\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "Create a storage bucket to store intermediate artifacts such as datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bucket",
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://21f1000629_mlops-oppe-1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "**If your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KdZ_IROfaOov",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://21f1000629_mlops-oppe-1/...\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3330b4f12a0d"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "e088ea8cd4a0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3938f6d37a1"
   },
   "source": [
    "### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "e95ca1e5e07c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Configure resource names\n",
    "\n",
    "Set a name for the following parameters:\n",
    "\n",
    "`MODEL_ARTIFACT_DIR` - Folder directory path to your model artifacts within a Cloud Storage bucket, for example: \"my-models/fraud-detection/trial-4\"\n",
    "\n",
    "`REPOSITORY` - Name of the Artifact Repository to create or use.\n",
    "\n",
    "`IMAGE` - Name of the container image that is pushed to the repository.\n",
    "\n",
    "`MODEL_DISPLAY_NAME` - Display name of Vertex AI model resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MzGDU7TWdts_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ARTIFACT_DIR = \"my-models/stock-prediction-classifier\"  # @param {type:\"string\"}\n",
    "REPOSITORY = \"stock-classifier-repo\"  # @param {type:\"string\"}\n",
    "IMAGE = \"stock-classifier-img\"  # @param {type:\"string\"}\n",
    "MODEL_DISPLAY_NAME = \"stock-classifier\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the defaults if no names were specified\n",
    "if MODEL_ARTIFACT_DIR == \"[your-artifact-directory]\":\n",
    "    MODEL_ARTIFACT_DIR = \"custom-container-prediction-model\"\n",
    "\n",
    "if REPOSITORY == \"[your-repository-name]\":\n",
    "    REPOSITORY = \"custom-container-prediction\"\n",
    "\n",
    "if IMAGE == \"[your-image-name]\":\n",
    "    IMAGE = \"sklearn-fastapi-server\"\n",
    "\n",
    "if MODEL_DISPLAY_NAME == \"[your-model-display-name]\":\n",
    "    MODEL_DISPLAY_NAME = \"sklearn-custom-container\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3c2d091d9e73"
   },
   "source": [
    "## Simple clasifier model\n",
    "Build a classifier model on stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QrZs7rX7aOox",
    "outputId": "13800365-b2f0-415d-c1ba-12f1140147c5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-02 09:15:00+05:30</td>\n",
       "      <td>340.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-02 09:16:00+05:30</td>\n",
       "      <td>340.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-02 09:17:00+05:30</td>\n",
       "      <td>340.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-02 09:18:00+05:30</td>\n",
       "      <td>340.0</td>\n",
       "      <td>343.7</td>\n",
       "      <td>340.0</td>\n",
       "      <td>343.7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-02 09:19:00+05:30</td>\n",
       "      <td>343.7</td>\n",
       "      <td>343.7</td>\n",
       "      <td>343.7</td>\n",
       "      <td>343.7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   timestamp   open   high    low  close  volume\n",
       "0  2017-01-02 09:15:00+05:30  340.0  340.0  340.0  340.0    11.0\n",
       "1  2017-01-02 09:16:00+05:30  340.0  340.0  340.0  340.0     0.0\n",
       "2  2017-01-02 09:17:00+05:30  340.0  340.0  340.0  340.0     0.0\n",
       "3  2017-01-02 09:18:00+05:30  340.0  343.7  340.0  343.7     1.0\n",
       "4  2017-01-02 09:19:00+05:30  343.7  343.7  343.7  343.7     1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas.plotting import parallel_coordinates\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn import metrics\n",
    "\n",
    "data = pd.read_csv('Stock_prediction_pipeline/data/v0/AARTIIND__EQ__NSE__NSE__MINUTE.csv')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "pfAmdVpsaOoy",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done. Cleaned data saved to: Stock_prediction_pipeline/data/processed/v0_processed.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# === Set up paths ===\n",
    "DATA_DIR = \"Stock_prediction_pipeline/data/v0\"\n",
    "FILES = [\"AARTIIND__EQ__NSE__NSE__MINUTE.csv\", \"ABCAPITAL__EQ__NSE__NSE__MINUTE.csv\"]\n",
    "OUTPUT_DIR = \"Stock_prediction_pipeline/data/processed\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def load_and_prepare_stock(file_name):\n",
    "    stock_name = file_name.split(\"__\")[0]\n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, file_name))\n",
    "\n",
    "    # Convert timestamp and sort\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.sort_values('timestamp')\n",
    "    df['stock'] = stock_name\n",
    "\n",
    "    # Fill missing minutes\n",
    "    full_range = pd.date_range(start=df['timestamp'].min(), end=df['timestamp'].max(), freq='min')\n",
    "\n",
    "    full_df = pd.DataFrame({'timestamp': full_range})\n",
    "    df = pd.merge(full_df, df, on='timestamp', how='left')\n",
    "    df['stock'] = df['stock'].fillna(stock_name)\n",
    "    df[['open', 'high', 'low', 'close', 'volume']] = df[['open', 'high', 'low', 'close', 'volume']].ffill()\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Feature engineering\n",
    "    df['rolling_avg_10'] = df['close'].rolling(window=10, min_periods=10).mean()\n",
    "    df['volume_sum_10'] = df['volume'].rolling(window=10, min_periods=10).sum()\n",
    "\n",
    "    # Target generation\n",
    "    df['future_close'] = df['close'].shift(-5)\n",
    "    df['target'] = (df['future_close'] > df['close']).astype(int)\n",
    "    df = df.drop(columns=['future_close'])\n",
    "\n",
    "    # Drop incomplete rows\n",
    "    df = df.dropna(subset=['rolling_avg_10', 'volume_sum_10'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# === Process both stocks ===\n",
    "combined_df = pd.concat([load_and_prepare_stock(f) for f in FILES], ignore_index=True)\n",
    "\n",
    "# Shuffle for modeling\n",
    "combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save result\n",
    "output_path = os.path.join(OUTPUT_DIR, \"v0_processed.csv\")\n",
    "combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"✅ Done. Cleaned data saved to:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Jo4sttq8aOoy",
    "outputId": "b223ad85-27e8-4531-c3cb-dfa22390c3df",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train-test split done.\n",
      "Train shape: (3085618, 10)\n",
      "Test shape: (771405, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the processed data\n",
    "processed_path = \"Stock_prediction_pipeline/data/processed/v0_processed.csv\"\n",
    "df = pd.read_csv(processed_path)\n",
    "\n",
    "# Train-test split\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Save the splits\n",
    "train_path = \"Stock_prediction_pipeline/data/processed/train.csv\"\n",
    "test_path = \"Stock_prediction_pipeline/data/processed/test.csv\"\n",
    "\n",
    "train_df.to_csv(train_path, index=False)\n",
    "test_df.to_csv(test_path, index=False)\n",
    "\n",
    "print(\"✅ Train-test split done.\")\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done. Cleaned data saved to: Stock_prediction_pipeline/data/processed/v1_processed.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# === Set up paths ===\n",
    "DATA_DIRS = [\"Stock_prediction_pipeline/data/v0\", \"Stock_prediction_pipeline/data/v1\"]\n",
    "OUTPUT_DIR = \"Stock_prediction_pipeline/data/processed\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# === Get all CSV files from both folders ===\n",
    "FILES = []\n",
    "for directory in DATA_DIRS:\n",
    "    FILES.extend(glob.glob(f\"{directory}/*.csv\"))\n",
    "\n",
    "def load_and_prepare_stock(file_path):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    stock_name = file_name.split(\"__\")[0]\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert timestamp and sort\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.sort_values('timestamp')\n",
    "    df['stock'] = stock_name\n",
    "\n",
    "    # Fill missing minutes\n",
    "    full_range = pd.date_range(start=df['timestamp'].min(), end=df['timestamp'].max(), freq='min')\n",
    "    full_df = pd.DataFrame({'timestamp': full_range})\n",
    "    df = pd.merge(full_df, df, on='timestamp', how='left')\n",
    "    df['stock'] = df['stock'].fillna(stock_name)\n",
    "    df[['open', 'high', 'low', 'close', 'volume']] = df[['open', 'high', 'low', 'close', 'volume']].ffill()\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Feature engineering\n",
    "    df['rolling_avg_10'] = df['close'].rolling(window=10, min_periods=10).mean()\n",
    "    df['volume_sum_10'] = df['volume'].rolling(window=10, min_periods=10).sum()\n",
    "\n",
    "    # Target generation\n",
    "    df['future_close'] = df['close'].shift(-5)\n",
    "    df['target'] = (df['future_close'] > df['close']).astype(int)\n",
    "    df = df.drop(columns=['future_close'])\n",
    "\n",
    "    # Drop incomplete rows\n",
    "    df = df.dropna(subset=['rolling_avg_10', 'volume_sum_10'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# === Process all stocks ===\n",
    "combined_df = pd.concat([load_and_prepare_stock(f) for f in FILES], ignore_index=True)\n",
    "\n",
    "# Shuffle for modeling\n",
    "combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save result for v1\n",
    "output_path = os.path.join(OUTPUT_DIR, \"v1_processed.csv\")\n",
    "combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"✅ Done. Cleaned data saved to:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ v1 Train-test split done.\n",
      "v1 Train shape: (7358089, 10)\n",
      "v1 Test shape: (1839523, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the v1 processed data\n",
    "processed_path = \"Stock_prediction_pipeline/data/processed/v1_processed.csv\"\n",
    "df = pd.read_csv(processed_path)\n",
    "\n",
    "# Train-test split for v1\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Save the splits for v1\n",
    "train_path = \"Stock_prediction_pipeline/data/processed/train_v1.csv\"\n",
    "test_path = \"Stock_prediction_pipeline/data/processed/test_v1.csv\"\n",
    "\n",
    "train_df.to_csv(train_path, index=False)\n",
    "test_df.to_csv(test_path, index=False)\n",
    "\n",
    "print(\"✅ v1 Train-test split done.\")\n",
    "print(f\"v1 Train shape: {train_df.shape}\")\n",
    "print(f\"v1 Test shape: {test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fA2Nv6waaOoz",
    "outputId": "33840994-485e-4068-a4f4-f89df40d01c9",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artifacts/model.joblib']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pickle\n",
    "# import joblib\n",
    "\n",
    "# joblib.dump(mod_dt, \"artifacts/model.joblib\")  #do this later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3849066a33bd"
   },
   "source": [
    "### Upload model artifacts and custom code to Cloud Storage\n",
    "\n",
    "Before you can deploy your model for serving, Vertex AI needs access to the following files in Cloud Storage:\n",
    "\n",
    "* `model.joblib` (model artifact)\n",
    "* `preprocessor.pkl` (model artifact)\n",
    "\n",
    "Run the following commands to upload your files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ca67ee52d4d9",
    "outputId": "c0a25974-fb26-42bf-9753-7606bce2d977",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://artifacts/model.joblib [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  2.6 KiB/  2.6 KiB]                                                \n",
      "Operation completed over 1 objects/2.6 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "# !gsutil cp artifacts/model.joblib {BUCKET_URI}/{MODEL_ARTIFACT_DIR}/ # do this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/Stock_prediction_pipeline\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load CSV\n",
    "\n",
    "# df = pd.read_csv(\"/home/jupyter/Stock_prediction_pipeline/data/processed/train.csv\")\n",
    "\n",
    "# # Save as Parquet\n",
    "# df.to_parquet(\"/home/jupyter/Stock_prediction_pipeline/data/processed/train.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/home/jupyter/Stock_prediction_pipeline/data/processed/v1_processed.csv\")\n",
    "df.to_parquet(\"/home/jupyter/Stock_prediction_pipeline/data/processed/v1_processed.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-01-02 09:24:00+05:30 2021-01-01 15:29:00+05:30\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"/home/jupyter/Stock_prediction_pipeline/data/processed/v1_processed.parquet\")\n",
    "print(df['timestamp'].min(), df['timestamp'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"/home/jupyter/Stock_prediction_pipeline/data/processed/v1_processed.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: Convert to datetime and force UTC awareness if needed\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "# Step 2: Convert from UTC to IST (Asia/Kolkata)\n",
    "df[\"timestamp\"] = df[\"timestamp\"].dt.tz_convert(\"Asia/Kolkata\")\n",
    "\n",
    "# Optional: If Feast prefers UTC\n",
    "# df[\"timestamp\"] = df[\"timestamp\"].dt.tz_convert(\"UTC\")\n",
    "\n",
    "# Save to Parquet\n",
    "df.to_parquet(\"/home/jupyter/Stock_prediction_pipeline/data/processed/v1_processed.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mv /home/jupyter/Stock_prediction_pipeline/data/processed/v1_processed.parquet /home/jupyter/Stock_prediction_pipeline/feature_repo/data/v1_processed.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
